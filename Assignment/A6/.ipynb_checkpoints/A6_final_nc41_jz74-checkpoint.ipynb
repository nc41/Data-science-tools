{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ck0HQoAFMZr3"
   },
   "source": [
    "COMP 543 A6\n",
    "Nai-Fan Chen (nc41)\n",
    "Jialei Zhou (jz74)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "SwPoQ1QfktA6",
    "outputId": "d4112a8b-aeb0-4705-b9a4-287f86f03fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9980 Loss 0.9637191 Correct 126 out of 200\n",
      "Step 9981 Loss 0.93740684 Correct 128 out of 200\n",
      "Step 9982 Loss 1.092393 Correct 121 out of 200\n",
      "Step 9983 Loss 1.0634432 Correct 107 out of 200\n",
      "Step 9984 Loss 1.0598913 Correct 106 out of 200\n",
      "Step 9985 Loss 1.0459412 Correct 120 out of 200\n",
      "Step 9986 Loss 1.0308971 Correct 120 out of 200\n",
      "Step 9987 Loss 1.0298545 Correct 117 out of 200\n",
      "Step 9988 Loss 1.1359423 Correct 108 out of 200\n",
      "Step 9989 Loss 1.0604455 Correct 118 out of 200\n",
      "Step 9990 Loss 0.9990126 Correct 121 out of 200\n",
      "Step 9991 Loss 0.9842941 Correct 124 out of 200\n",
      "Step 9992 Loss 1.0265788 Correct 123 out of 200\n",
      "Step 9993 Loss 1.0259724 Correct 116 out of 200\n",
      "Step 9994 Loss 1.0228249 Correct 118 out of 200\n",
      "Step 9995 Loss 1.0521476 Correct 112 out of 200\n",
      "Step 9996 Loss 1.0664648 Correct 127 out of 200\n",
      "Step 9997 Loss 1.0461231 Correct 109 out of 200\n",
      "Step 9998 Loss 1.0624418 Correct 113 out of 200\n",
      "Step 9999 Loss 1.0065873 Correct 119 out of 200\n",
      "Average loss for the last 10 mini-batches is 1.029244738817215 average correct labels is 118.2 out of 200 .\n",
      "Accuracy(%): 0.591\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "# the number of iterations to train for\n",
    "numTrainingIters = 10000\n",
    "# the number of hidden neurons that hold the state of the RNN\n",
    "hiddenUnits = 200\n",
    "\n",
    "# the number of classes that we are learning over\n",
    "numClasses = 5\n",
    "\n",
    "# the number of data points in a batch\n",
    "batchSize = 200\n",
    "\n",
    "# the learning rate\n",
    "learningRate = 0.01\n",
    "\n",
    "# this function takes a dictionary (called data) which contains \n",
    "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
    "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
    "# an ascii character, and the sequence of vectors corresponds to\n",
    "# one line of text.  classNumber indicates which file the line of\n",
    "# text came from.  \n",
    "# \n",
    "# The argument maxSeqLen is the maximum length of a line of text\n",
    "# seen so far.  fileName is the name of a file whose contents\n",
    "# we want to add to data.  classNum is an indicator of the class\n",
    "# we are going to associate with text from that file.  linesToUse\n",
    "# tells us how many lines to sample from the file.\n",
    "#\n",
    "# The return val is the new maxSeqLen, as well as the new data\n",
    "# dictionary with the additional lines of text added\n",
    "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
    "    #\n",
    "    # open the file and read it in\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    #\n",
    "    # sample linesToUse numbers; these will tell us what lines\n",
    "    # from the text file we will use\n",
    "    # [Note] random_integers genetate a vector with size \"linesToUse\", rand from 0 to len(content)\n",
    "    myInts = np.random.randint (0, len(content) - 1, linesToUse)\n",
    "    #\n",
    "    # i is the key of the next line of text to add to the dictionary\n",
    "    # [Note] dictionary is called \"data\" in this case, so i is the length of dictionary\n",
    "    i = len(data)\n",
    "    #\n",
    "    # loop thru and add the lines of text to the dictionary\n",
    "    for whichLine in myInts.flat: # myInts.flat is a 1-D interator over myInts\n",
    "        #\n",
    "        # get the line and ignore it if it has nothing in it\n",
    "        line = content[whichLine]\n",
    "        if line.isspace () or len(line) == 0:\n",
    "            continue;\n",
    "        #\n",
    "        # take note if this is the longest line we've seen\n",
    "        if len (line) > maxSeqLen:\n",
    "            maxSeqLen = len (line)\n",
    "        #\n",
    "        # create the matrix that will hold this line\n",
    "        temp = np.zeros((len(line), 256))\n",
    "        #\n",
    "        # j is the character we are on\n",
    "        j = 0\n",
    "        # \n",
    "        # loop thru the characters\n",
    "        for ch in line:\n",
    "            #\n",
    "            # non-ascii? ignore\n",
    "            if ord(ch) >= 256: # ord(c) gives the unicode of c\n",
    "                continue\n",
    "            #\n",
    "            # one hot!\n",
    "            temp[j][ord(ch)] = 1 # mark the ascii index \n",
    "            # \n",
    "            # move onto the next character\n",
    "            j = j + 1\n",
    "            #\n",
    "        # remember the line of text\n",
    "        # add this (class number, matrix_of_line) to end of data (dictionary)\n",
    "        data[i] = (classNum, temp)\n",
    "        #\n",
    "        # move onto the next line\n",
    "        i = i + 1\n",
    "    #\n",
    "    # and return the dictionary with the new data\n",
    "    return (maxSeqLen, data) # (max length of the line in file, and the dictionary)\n",
    "\n",
    "# this function takes as input a data set encoded as a dictionary\n",
    "# (same encoding as the last function) and pre-pends every line of\n",
    "# text with empty characters so that each line of text is exactly\n",
    "# maxSeqLen characters in size\n",
    "def pad (maxSeqLen, data):\n",
    "   #\n",
    "   # loop thru every line of text\n",
    "   for i in data:\n",
    "        #\n",
    "        # access the matrix and the label\n",
    "        temp = data[i][1]\n",
    "        label = data[i][0]\n",
    "        # \n",
    "        # get the number of chatacters in this line\n",
    "        len = temp.shape[0]\n",
    "        #\n",
    "        # and then pad so the line is the correct length\n",
    "        padding = np.zeros ((maxSeqLen - len,256)) \n",
    "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
    "   #\n",
    "   # return the new data set\n",
    "   return data\n",
    "\n",
    "# this generates a new batch of training data of size batchSize from the\n",
    "# list of lines of text data. This version of generateData is useful for\n",
    "# an RNN because the data set x is a NumPy array with dimensions\n",
    "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
    "# matrices containing one-hot character encodings for each data point\n",
    "# using tf.unstack(inputX, axis=2)\n",
    "def generateDataRNN (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1] for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# this also generates a new batch of training data, but it represents\n",
    "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
    "# where for each data point, all characters have been appended.  Useful\n",
    "# for feed-forward network training\n",
    "def generateDataFeedForward (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1].flatten () for i in myInts.flat) # flatten turns matrix into 1-D form\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# create the data dictionary\n",
    "maxSeqLen = 0\n",
    "data = {}\n",
    "\n",
    "# load up the five data sets\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"biochemistry_processed.txt\", 0, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"cancerResearch_processed.txt\", 1, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"jama_processed.txt\", 2, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"nature_processed.txt\", 3, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"plosOne_processed.txt\", 4, 10000)\n",
    "\n",
    "# pad each entry in the dictionary with empty characters as needed so\n",
    "# that the sequences are all of the same length\n",
    "data = pad (maxSeqLen, data)\n",
    "        \n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (hiddenUnits*2 + 256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "# now we implement the forward pass\n",
    "currentState = initialState\n",
    "\n",
    "oldState = collections.deque()\n",
    "oldState.append(currentState)\n",
    "padState = tf.Variable(np.zeros((batchSize,hiddenUnits)), dtype=tf.float32)\n",
    "for timeTick in sequenceOfLetters:\n",
    "    #\n",
    "    # concatenate the state with the input, then compute the next state\n",
    "    if len(oldState) <= 10:\n",
    "      inputPlusState = tf.concat([timeTick, currentState, padState], 1) \n",
    "    else:\n",
    "      inputPlusState = tf.concat([timeTick, currentState, oldState.popleft()], 1)\n",
    "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "    currentState = next_state\n",
    "    oldState.append(currentState)\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2 # matmul\n",
    "\n",
    "predictions = tf.nn.softmax(outputs) # softmax\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(learningRate).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "# trainingAlg = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "# trainingAlg = tf.train.AdadeltaOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    _accLoss = 0.0\n",
    "    _accCount = 0\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "#         if epoch%100 == 0: \n",
    "        if epoch >= numTrainingIters - 20: \n",
    "            print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "\n",
    "        if epoch >= numTrainingIters - 10:\n",
    "            _accLoss += _totalLoss\n",
    "            _accCount += numCorrect \n",
    "    \n",
    "    print(\"Average loss for the last 10 mini-batches is\", _accLoss / 10, \n",
    "          \"average correct labels is\", _accCount / 10, \"out of\", batchSize, '.')\n",
    "    print(\"Accuracy(%):\", (_accCount/10) / batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "RU9vjpvOkifS",
    "outputId": "3cb2e22f-d941-4f4a-ba7f-19ff1ee098a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9980 Loss 0.10319737 Correct 147 out of 150\n",
      "Step 9981 Loss 0.033834197 Correct 148 out of 150\n",
      "Step 9982 Loss 0.17667341 Correct 144 out of 150\n",
      "Step 9983 Loss 0.21053162 Correct 144 out of 150\n",
      "Step 9984 Loss 0.11489423 Correct 147 out of 150\n",
      "Step 9985 Loss 0.13874622 Correct 145 out of 150\n",
      "Step 9986 Loss 0.1746055 Correct 144 out of 150\n",
      "Step 9987 Loss 0.1351292 Correct 145 out of 150\n",
      "Step 9988 Loss 0.15116279 Correct 145 out of 150\n",
      "Step 9989 Loss 0.14264633 Correct 147 out of 150\n",
      "Step 9990 Loss 0.1256092 Correct 146 out of 150\n",
      "Step 9991 Loss 0.08521423 Correct 145 out of 150\n",
      "Step 9992 Loss 0.22226799 Correct 142 out of 150\n",
      "Step 9993 Loss 0.13218355 Correct 145 out of 150\n",
      "Step 9994 Loss 0.17884059 Correct 146 out of 150\n",
      "Step 9995 Loss 0.14067146 Correct 144 out of 150\n",
      "Step 9996 Loss 0.17855258 Correct 145 out of 150\n",
      "Step 9997 Loss 0.23175618 Correct 146 out of 150\n",
      "Step 9998 Loss 0.17987251 Correct 146 out of 150\n",
      "Step 9999 Loss 0.17949745 Correct 146 out of 150\n",
      "Average loss for the last 10 mini-batches is 0.16544657498598098 average correct labels is 145.1 out of 150.\n",
      "Accuracy(%): 0.9673333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "# the number of iterations to train for\n",
    "numTrainingIters = 10000\n",
    "# the number of hidden neurons that hold the state of the RNN\n",
    "hiddenUnits = 1000\n",
    "\n",
    "# the number of classes that we are learning over\n",
    "numClasses = 5\n",
    "\n",
    "# the number of data points in a batch\n",
    "batchSize = 150\n",
    "\n",
    "# the learning rate\n",
    "learningRate = 0.01\n",
    "\n",
    "# this function takes a dictionary (called data) which contains \n",
    "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
    "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
    "# an ascii character, and the sequence of vectors corresponds to\n",
    "# one line of text.  classNumber indicates which file the line of\n",
    "# text came from.  \n",
    "# \n",
    "# The argument maxSeqLen is the maximum length of a line of text\n",
    "# seen so far.  fileName is the name of a file whose contents\n",
    "# we want to add to data.  classNum is an indicator of the class\n",
    "# we are going to associate with text from that file.  linesToUse\n",
    "# tells us how many lines to sample from the file.\n",
    "#\n",
    "# The return val is the new maxSeqLen, as well as the new data\n",
    "# dictionary with the additional lines of text added\n",
    "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
    "    #\n",
    "    # open the file and read it in\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    #\n",
    "    # sample linesToUse numbers; these will tell us what lines\n",
    "    # from the text file we will use\n",
    "    # [Note] random_integers genetate a vector with size \"linesToUse\", rand from 0 to len(content)\n",
    "    myInts = np.random.randint (0, len(content) - 1, linesToUse)\n",
    "    #\n",
    "    # i is the key of the next line of text to add to the dictionary\n",
    "    # [Note] dictionary is called \"data\" in this case, so i is the length of dictionary\n",
    "    i = len(data)\n",
    "    #\n",
    "    # loop thru and add the lines of text to the dictionary\n",
    "    for whichLine in myInts.flat: # myInts.flat is a 1-D interator over myInts\n",
    "        #\n",
    "        # get the line and ignore it if it has nothing in it\n",
    "        line = content[whichLine]\n",
    "        if line.isspace () or len(line) == 0:\n",
    "            continue;\n",
    "        #\n",
    "        # take note if this is the longest line we've seen\n",
    "        if len (line) > maxSeqLen:\n",
    "            maxSeqLen = len (line)\n",
    "        #\n",
    "        # create the matrix that will hold this line\n",
    "        temp = np.zeros((len(line), 256))\n",
    "        #\n",
    "        # j is the character we are on\n",
    "        j = 0\n",
    "        # \n",
    "        # loop thru the characters\n",
    "        for ch in line:\n",
    "            #\n",
    "            # non-ascii? ignore\n",
    "            if ord(ch) >= 256: # ord(c) gives the unicode of c\n",
    "                continue\n",
    "            #\n",
    "            # one hot!\n",
    "            temp[j][ord(ch)] = 1 # mark the ascii index \n",
    "            # \n",
    "            # move onto the next character\n",
    "            j = j + 1\n",
    "            #\n",
    "        # remember the line of text\n",
    "        # add this (class number, matrix_of_line) to end of data (dictionary)\n",
    "        data[i] = (classNum, temp)\n",
    "        #\n",
    "        # move onto the next line\n",
    "        i = i + 1\n",
    "    #\n",
    "    # and return the dictionary with the new data\n",
    "    return (maxSeqLen, data) # (max length of the line in file, and the dictionary)\n",
    "\n",
    "# this function takes as input a data set encoded as a dictionary\n",
    "# (same encoding as the last function) and pre-pends every line of\n",
    "# text with empty characters so that each line of text is exactly\n",
    "# maxSeqLen characters in size\n",
    "def pad (maxSeqLen, data):\n",
    "   #\n",
    "   # loop thru every line of text\n",
    "   for i in data:\n",
    "        #\n",
    "        # access the matrix and the label\n",
    "        temp = data[i][1]\n",
    "        label = data[i][0]\n",
    "        # \n",
    "        # get the number of chatacters in this line\n",
    "        len = temp.shape[0]\n",
    "        #\n",
    "        # and then pad so the line is the correct length\n",
    "        padding = np.zeros ((maxSeqLen - len,256)) \n",
    "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
    "   #\n",
    "   # return the new data set\n",
    "   return data\n",
    "\n",
    "# this generates a new batch of training data of size batchSize from the\n",
    "# list of lines of text data. This version of generateData is useful for\n",
    "# an RNN because the data set x is a NumPy array with dimensions\n",
    "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
    "# matrices containing one-hot character encodings for each data point\n",
    "# using tf.unstack(inputX, axis=2)\n",
    "def generateDataRNN (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1] for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# this also generates a new batch of training data, but it represents\n",
    "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
    "# where for each data point, all characters have been appended.  Useful\n",
    "# for feed-forward network training\n",
    "def generateDataFeedForward (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1].flatten () for i in myInts.flat) # flatten turns matrix into 1-D form\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# create the data dictionary\n",
    "maxSeqLen = 0\n",
    "data = {}\n",
    "\n",
    "# load up the five data sets\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"biochemistry_processed.txt\", 0, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"cancerResearch_processed.txt\", 1, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"jama_processed.txt\", 2, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"nature_processed.txt\", 3, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"plosOne_processed.txt\", 4, 10000)\n",
    "\n",
    "# pad each entry in the dictionary with empty characters as needed so\n",
    "# that the sequences are all of the same length\n",
    "data = pad (maxSeqLen, data)\n",
    "        \n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256*maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (256*maxSeqLen, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "# sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "# now we implement the forward pass\n",
    "currentState = tf.tanh(tf.matmul(inputX, W) + b)\n",
    "\n",
    "# oldState = collections.deque()\n",
    "# oldState.append(currentState)\n",
    "# padState = tf.Variable(np.zeros((batchSize,hiddenUnits)), dtype=tf.float32)\n",
    "# for timeTick in sequenceOfLetters:\n",
    "#     #\n",
    "#     # concatenate the state with the input, then compute the next state\n",
    "#     if len(oldState) <= 10:\n",
    "#       inputPlusState = tf.concat([timeTick, currentState, padState], 1) \n",
    "#     else:\n",
    "#       inputPlusState = tf.concat([timeTick, currentState, oldState.popleft()], 1)\n",
    "#     next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "#     currentState = next_state\n",
    "#     oldState.append(currentState)\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2 # matmul\n",
    "\n",
    "predictions = tf.nn.softmax(outputs) # softmax\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(learningRate).minimize(totalLoss)\n",
    "# trainingAlg = tf.train.AdagradOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "# trainingAlg = tf.train.AdadeltaOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    _accLoss = 0.0\n",
    "    _accCount = 0\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataFeedForward(maxSeqLen, data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "#         if epoch%100 == 0: \n",
    "        if epoch >= numTrainingIters - 20: \n",
    "            print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "\n",
    "        if epoch >= numTrainingIters - 10:\n",
    "            _accLoss += _totalLoss\n",
    "            _accCount += numCorrect \n",
    "    \n",
    "    print(\"Average loss for the last 10 mini-batches is\", _accLoss / 10, \n",
    "          \"average correct labels is\", _accCount / 10, \"out of 150.\")\n",
    "    print(\"Accuracy(%):\", (_accCount/10) / batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "FRQm9yj4k4to",
    "outputId": "c2917cf2-ca5b-4372-d8c7-51a33f37f7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9980 Loss 1.1628346 Correct 72 out of 150\n",
      "Step 9981 Loss 1.1389586 Correct 74 out of 150\n",
      "Step 9982 Loss 1.2558727 Correct 70 out of 150\n",
      "Step 9983 Loss 1.1379296 Correct 74 out of 150\n",
      "Step 9984 Loss 1.0801747 Correct 88 out of 150\n",
      "Step 9985 Loss 1.1767743 Correct 83 out of 150\n",
      "Step 9986 Loss 1.1600622 Correct 76 out of 150\n",
      "Step 9987 Loss 1.1800702 Correct 83 out of 150\n",
      "Step 9988 Loss 1.1132063 Correct 87 out of 150\n",
      "Step 9989 Loss 1.085437 Correct 84 out of 150\n",
      "Step 9990 Loss 1.1571736 Correct 82 out of 150\n",
      "Step 9991 Loss 1.1632593 Correct 79 out of 150\n",
      "Step 9992 Loss 1.1279404 Correct 87 out of 150\n",
      "Step 9993 Loss 1.1347382 Correct 81 out of 150\n",
      "Step 9994 Loss 1.0850183 Correct 79 out of 150\n",
      "Step 9995 Loss 1.1645771 Correct 83 out of 150\n",
      "Step 9996 Loss 1.0962888 Correct 83 out of 150\n",
      "Step 9997 Loss 1.0850582 Correct 85 out of 150\n",
      "Step 9998 Loss 1.1552424 Correct 81 out of 150\n",
      "Step 9999 Loss 1.2171291 Correct 67 out of 150\n",
      "Average loss for the last 10 mini-batches is 1.1386425495147705 average correct labels is 80.7 out of 150.\n",
      "Accuracy(%): 0.538\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# the number of iterations to train for\n",
    "numTrainingIters = 10000\n",
    "\n",
    "# the number of hidden neurons that hold the state of the RNN\n",
    "hiddenUnits = 800\n",
    "\n",
    "# the number of classes that we are learning over\n",
    "numClasses = 5\n",
    "\n",
    "# the number of data points in a batch\n",
    "batchSize = 150\n",
    "\n",
    "# the learning rate\n",
    "learningRate = 0.01\n",
    "\n",
    "timesteps = 28\n",
    "\n",
    "\n",
    "# this function takes a dictionary (called data) which contains \n",
    "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
    "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
    "# an ascii character, and the sequence of vectors corresponds to\n",
    "# one line of text.  classNumber indicates which file the line of\n",
    "# text came from.  \n",
    "# \n",
    "# The argument maxSeqLen is the maximum length of a line of text\n",
    "# seen so far.  fileName is the name of a file whose contents\n",
    "# we want to add to data.  classNum is an indicator of the class\n",
    "# we are going to associate with text from that file.  linesToUse\n",
    "# tells us how many lines to sample from the file.\n",
    "#\n",
    "# The return val is the new maxSeqLen, as well as the new data\n",
    "# dictionary with the additional lines of text added\n",
    "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
    "    #\n",
    "    # open the file and read it in\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    #\n",
    "    # sample linesToUse numbers; these will tell us what lines\n",
    "    # from the text file we will use\n",
    "    # [Note] random_integers genetate a vector with size \"linesToUse\", rand from 0 to len(content)\n",
    "    myInts = np.random.randint (0, len(content) - 1, linesToUse)\n",
    "    #\n",
    "    # i is the key of the next line of text to add to the dictionary\n",
    "    # [Note] dictionary is called \"data\" in this case, so i is the length of dictionary\n",
    "    i = len(data)\n",
    "    #\n",
    "    # loop thru and add the lines of text to the dictionary\n",
    "    for whichLine in myInts.flat: # myInts.flat is a 1-D interator over myInts\n",
    "        #\n",
    "        # get the line and ignore it if it has nothing in it\n",
    "        line = content[whichLine]\n",
    "        if line.isspace () or len(line) == 0:\n",
    "            continue;\n",
    "        #\n",
    "        # take note if this is the longest line we've seen\n",
    "        if len (line) > maxSeqLen:\n",
    "            maxSeqLen = len (line)\n",
    "        #\n",
    "        # create the matrix that will hold this line\n",
    "        temp = np.zeros((len(line), 256))\n",
    "        #\n",
    "        # j is the character we are on\n",
    "        j = 0\n",
    "        # \n",
    "        # loop thru the characters\n",
    "        for ch in line:\n",
    "            #\n",
    "            # non-ascii? ignore\n",
    "            if ord(ch) >= 256: # ord(c) gives the unicode of c\n",
    "                continue\n",
    "            #\n",
    "            # one hot!\n",
    "            temp[j][ord(ch)] = 1 # mark the ascii index \n",
    "            # \n",
    "            # move onto the next character\n",
    "            j = j + 1\n",
    "            #\n",
    "        # remember the line of text\n",
    "        # add this (class number, matrix_of_line) to end of data (dictionary)\n",
    "        data[i] = (classNum, temp)\n",
    "        #\n",
    "        # move onto the next line\n",
    "        i = i + 1\n",
    "    #\n",
    "    # and return the dictionary with the new data\n",
    "    return (maxSeqLen, data) # (max length of the line in file, and the dictionary)\n",
    "\n",
    "# this function takes as input a data set encoded as a dictionary\n",
    "# (same encoding as the last function) and pre-pends every line of\n",
    "# text with empty characters so that each line of text is exactly\n",
    "# maxSeqLen characters in size\n",
    "def pad (maxSeqLen, data):\n",
    "   #\n",
    "   # loop thru every line of text\n",
    "   for i in data:\n",
    "        #\n",
    "        # access the matrix and the label\n",
    "        temp = data[i][1]\n",
    "        label = data[i][0]\n",
    "        # \n",
    "        # get the number of chatacters in this line\n",
    "        len = temp.shape[0]\n",
    "        #\n",
    "        # and then pad so the line is the correct length\n",
    "        padding = np.zeros ((maxSeqLen - len,256)) \n",
    "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
    "   #\n",
    "   # return the new data set\n",
    "   return data\n",
    "\n",
    "# this generates a new batch of training data of size batchSize from the\n",
    "# list of lines of text data. This version of generateData is useful for\n",
    "# an RNN because the data set x is a NumPy array with dimensions\n",
    "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
    "# matrices containing one-hot character encodings for each data point\n",
    "# using tf.unstack(inputX, axis=2)\n",
    "def generateDataRNN (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1] for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# this also generates a new batch of training data, but it represents\n",
    "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
    "# where for each data point, all characters have been appended.  Useful\n",
    "# for feed-forward network training\n",
    "def generateDataFeedForward (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.randint (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1].flatten () for i in myInts.flat) # flatten turns matrix into 1-D form\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# create the data dictionary\n",
    "maxSeqLen = 0\n",
    "data = {}\n",
    "\n",
    "# load up the five data sets\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"biochemistry_processed.txt\", 0, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"cancerResearch_processed.txt\", 1, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"jama_processed.txt\", 2, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"nature_processed.txt\", 3, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"plosOne_processed.txt\", 4, 10000)\n",
    "\n",
    "# pad each entry in the dictionary with empty characters as needed so\n",
    "# that the sequences are all of the same length\n",
    "data = pad (maxSeqLen, data)\n",
    "        \n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (hiddenUnits + 256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenUnits, forget_bias=0.8)\n",
    "outputs, states = tf.nn.static_rnn(lstm_cell, sequenceOfLetters, dtype=tf.float32)\n",
    "\n",
    "logits = tf.matmul(outputs[-1], W2) + b2 # matmul\n",
    "\n",
    "predictions = tf.nn.softmax(logits) # softmax\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "# trainingAlg = tf.train.GradientDescentOptimizer(learningRate).minimize(totalLoss)\n",
    "# trainingAlg = tf.train.AdagradOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(totalLoss)\n",
    "\n",
    "# and train!!\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    _accLoss = 0.0\n",
    "    _accCount = 0\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                })\n",
    "        #\n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        #\n",
    "        # print out to the screen\n",
    "#         if epoch%100 == 0: \n",
    "        if epoch >= numTrainingIters - 20: \n",
    "            print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "\n",
    "        if epoch >= numTrainingIters - 10:\n",
    "            _accLoss += _totalLoss\n",
    "            _accCount += numCorrect \n",
    "    \n",
    "    print(\"Average loss for the last 10 mini-batches is\", _accLoss / 10, \n",
    "          \"average correct labels is\", _accCount / 10, \"out of 150.\")\n",
    "    print(\"Accuracy(%):\", (_accCount/10) / batchSize)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A6_final_nc41_jz74.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
